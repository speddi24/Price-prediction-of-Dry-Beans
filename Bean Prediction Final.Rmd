---
title: "Final_Var_Exp_NNet_John"
author: "John Herbert"
output:
  pdf_document: default
  word_document: default
urlcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message=F,warning=F,echo=F,fig_height=10,fig_width=7,cache = F)
```

## Document Libraries

* **knitr** package used for *kable* function used to format tables
* **ggplot2** package for graphs
* **gridExtra** package for formatting multiple **ggplot2** graphs
* **GGally** package for pair plots in **ggplot2** formatting
* **dplyr** package for restructuring data frames
* **tidyverse** package for restructuring data frames
* **mlbench** package
* **gridExtra** package for formatting multiple **ggplot2** graphs
* **randomForest** package used for feature selection modeling
* **NeuralNetTools** package for Neural Network plotting
* **doParallel** package for parallel processing on all cores
* **caret** package for neural network model, cross validation, and bootstrapping
* **NeuralNetTools** package for plotting nueral network structures

```{r}
# install.packages('knitr')
suppressWarnings(suppressMessages(library(knitr)))
# install.packages('ggplot2')
suppressWarnings(suppressMessages(library(ggplot2)))
# install.packages('gridExtra')
suppressWarnings(suppressMessages(library(gridExtra)))
# install.packages('GGally')
suppressWarnings(suppressMessages(library(GGally)))
# install.packages('dplyr')
suppressWarnings(suppressMessages(library(dplyr)))
# install.packages('tidyr')
suppressWarnings(suppressMessages(library(tidyr)))
# install.packages('mlbench')
suppressWarnings(suppressMessages(library(mlbench))) 
# install.packages('caret')
suppressWarnings(suppressMessages(library(caret)))
# install.packages('MASS')
suppressWarnings(suppressMessages(library(MASS)))
# install.packages('nnet')
suppressWarnings(suppressMessages(library(nnet)))
# install.packages('randomForest')
suppressWarnings(suppressMessages(library(randomForest)))
# install.packages('NeuralNetTools')
suppressWarnings(suppressMessages(library(NeuralNetTools)))
# install.packages('nnet')
suppressWarnings(suppressMessages(library(nnet)))
# install.packages('car')
suppressWarnings(suppressMessages(library(car)))
# install.packages('devtools')
library(devtools)
source_url('https://gist.githubusercontent.com/Peque/41a9e20d6687f2f3108d/raw/85e14f3a292e126f1454864427e3a189c2fe33f3/nnet_plot_update.r')
# install.packages('doParallel')
suppressWarnings(suppressMessages(library(doParallel)))
cores=detectCores()
registerDoParallel(cores=cores)
# install.packages('foreach')
suppressWarnings(suppressMessages(library(foreach)))
```

### Background:

  Like any other country in the World Dry beans are most produced pulse in Turkey. These plants are very sensitive to the climatic conditions. Use of low-quality seeds in production yields lower quality produce even though best growing conditions are provided. Identification of the beans help the farmers to pick best quality bean for cultivation and receiving fair price in the market. Manual separation of the cultivated crop containing mixed genotypes of seeds is a difficult process and are brought to the market without being separated. This results in decrease in the value of the beans immensely in the market.
  
  Based on the morphometric characteristics of these beans, the Turkish Standards Institution divided the dry beans into Barbunya, Battal, Bombay, Cali, Dermason, Horoz, Tombul, Selnik and Seker classes. In this study, we will explore number of automatic methods using Machine Learning techniques to classify the beans from mixed population and estimate price per pound of the crop with little to no uncertainty on price per pound of the crop. For this purpose, we will use the features extracted from the images of variety of beans by Computer Vision System (Multiclass classification of dry beans using computer vision and machine learning techniques (Computers and Electronics in Agriculture, Volume 174, 2020), Murat Koklu and Ilker Ali Ozkan). Machine Learning models like Quadratic Discriminant Analysis (QDA), Multinomial Logistic Regression, Support Vector Machine (SVM) and Neural Networks classification models were created with 10-fold cross validation and performance metrics were compared. Then the price per pound of the sample is predicted using the best model.
  
### Data Sources:

  All the training and sample data provided were used. The beans were classified into 6 categories in all the datasets. 
  
The hypothetical price per pound and weight in grams per seed for all the seeds is listed below,
1. Bombay	($5.56/lb, 1.92 g/seed)
2. Cali	(6.02/lb,	0.61 g/seed)
3. Dermason	(1.98/lb,	0.28 g/seed)
4. Horoz	(2.43/lb,	0.52 g/seed)
5. Seker	(2.72/lb,	0.49 g/seed)
6. Sira	(5.40/lb,	0.38 g/seed)

## Importing Data & Calculating the Iteraction Variables

For our analysis of to determine the weight of 1 lb of beans, we imported 4 documents: *labeled.csv*, *samp.A.csv*, *samp.B.csv*, and *samp.C.csv*. Based on the 9 variables given (8 features, 1 target), we calculated 9 interaction variables fomr the study: *Multiclass classification of dry beans using computer vision and machine learning techniques* for better classification results. These variables are listed in the next section. 

```{r}
# Import data
seed.dat <- read.csv("labeled.csv")
samp.A <- read.csv("samp.A.csv")
samp.B <- read.csv("samp.B.csv")
samp.C <- read.csv("samp.C.csv")
seed.dat <- subset(seed.dat, select = -X)
samp.A <- subset(samp.A, select = -X)
samp.B <- subset(samp.B, select = -X)
samp.C <- subset(samp.C, select = -X)

seed.dat$Area <- as.numeric(seed.dat$Area)
samp.A$Area <- as.numeric(samp.A$Area)
samp.B$Area <- as.numeric(samp.B$Area)
samp.C$Area <- as.numeric(samp.C$Area)

seed.dat$Eccentricity <- as.numeric(seed.dat$Eccentricity)
samp.A$Eccentricity <- as.numeric(samp.A$Eccentricity)
samp.B$Eccentricity <- as.numeric(samp.B$Eccentricity)
samp.C$Eccentricity <- as.numeric(samp.C$Eccentricity)

seed.dat$ConvexArea <- as.numeric(seed.dat$ConvexArea)
samp.A$ConvexArea <- as.numeric(samp.A$ConvexArea)
samp.B$ConvexArea <- as.numeric(samp.B$ConvexArea)
samp.C$ConvexArea <- as.numeric(samp.C$ConvexArea)

seed.dat$Class<- as.factor(seed.dat$Class)

#Add additional features from "Computers and Electronics in Agriculture" page 4

#Aspect Ratio (K)= MajorAxisLength/MinorAxisLength
seed.dat$AspectRatio <- seed.dat$MajorAxisLength/seed.dat$MinorAxisLength
#Solidity = Area/ConvexArea
seed.dat$Solidity <- seed.dat$Area/seed.dat$ConvexArea
#Roundness = 4piArea / Perimeter^2
seed.dat$Roundness <- (4 * pi * seed.dat$Area) / seed.dat$Perimeter^2
#EquivalentDiameter = sqrt(4Area/pit)
seed.dat$EquivalentDiameter <- sqrt((4*seed.dat$Area) / pi)
#Compactness = Ed(EquivalentDiameter)/L(MajorAxisLength)
seed.dat$Compactness <- seed.dat$EquivalentDiameter/seed.dat$MajorAxisLength
#ShapeFactor1(SF1) = MajorAxisLength/Area
seed.dat$SF1 <- seed.dat$MajorAxisLength / seed.dat$Area
#ShapeFactor2(SF2) = MinorAxisLength/Area
seed.dat$SF2 <- seed.dat$MinorAxisLength / seed.dat$Area
#ShapeFactor3(SF3) = Area / (MajorAxisLength/2)^2 * pi
seed.dat$SF3 <- seed.dat$Area / ((seed.dat$MajorAxisLength/2)^2 * pi)
#ShapeFactor4(SF4) = Area / (MajorAxisLength/2)* (MinorAxisLength/2) * pi
seed.dat$SF4 <- seed.dat$Area / (seed.dat$MajorAxisLength/2 * 
                                   seed.dat$MinorAxisLength/2 * pi)


col_order<- c("Area", "Perimeter", "MajorAxisLength", "MinorAxisLength", 
              "AspectRatio", "Eccentricity", "ConvexArea", "EquivalentDiameter", 
              "Extent", "Solidity", "Roundness", "Compactness", "SF1", "SF2", 
              "SF3", "SF4", "Class")
seed.dat<- seed.dat[,col_order]
```
```{r}
#Add additional features from "Computers and Electronics in Agriculture" page 4

#Aspect Ratio (K)= MajorAxisLength/MinorAxisLength

samp.A$AspectRatio <- samp.A$MajorAxisLength/samp.A$MinorAxisLength
#Solidity = Area/ConvexArea
samp.A$Solidity <- samp.A$Area/samp.A$ConvexArea
#Roundness = 4piArea / Perimeter^2
samp.A$Roundness <- (4 * pi * samp.A$Area) / samp.A$Perimeter^2
#EquivalentDiameter = sqrt(4Area/pit)
samp.A$EquivalentDiameter <- sqrt((4*samp.A$Area) / pi)
#Compactness = Ed(EquivalentDiameter)/L(MajorAxisLength)
samp.A$Compactness <- samp.A$EquivalentDiameter/samp.A$MajorAxisLength
#ShapeFactor1(SF1) = MajorAxisLength/Area
samp.A$SF1 <- samp.A$MajorAxisLength / samp.A$Area
#ShapeFactor2(SF2) = MinorAxisLength/Area
samp.A$SF2 <- samp.A$MinorAxisLength / samp.A$Area
#ShapeFactor3(SF3) = Area / (MajorAxisLength/2)^2 * pi
samp.A$SF3 <- samp.A$Area / ((samp.A$MajorAxisLength/2)^2 * pi)
#ShapeFactor4(SF4) = Area / (MajorAxisLength/2)* (MinorAxisLength/2) * pi
samp.A$SF4 <- samp.A$Area / (samp.A$MajorAxisLength/2 * 
                                   samp.A$MinorAxisLength/2 * pi)


col_order<- c("Area", "Perimeter", "MajorAxisLength", "MinorAxisLength", 
              "AspectRatio", "Eccentricity", "ConvexArea", "EquivalentDiameter", 
              "Extent", "Solidity", "Roundness", "Compactness", "SF1", "SF2", 
              "SF3", "SF4")
sampA.dat <- samp.A[,col_order]

```
```{r}
#Add additional features from "Computers and Electronics in Agriculture" page 4

#Aspect Ratio (K)= MajorAxisLength/MinorAxisLength

samp.B$AspectRatio <- samp.B$MajorAxisLength/samp.B$MinorAxisLength
#Solidity = Area/ConvexArea
samp.B$Solidity <- samp.B$Area/samp.B$ConvexArea
#Roundness = 4piArea / Perimeter^2
samp.B$Roundness <- (4 * pi * samp.B$Area) / samp.B$Perimeter^2
#EquivalentDiameter = sqrt(4Area/pit)
samp.B$EquivalentDiameter <- sqrt((4*samp.B$Area) / pi)
#Compactness = Ed(EquivalentDiameter)/L(MajorAxisLength)
samp.B$Compactness <- samp.B$EquivalentDiameter/samp.B$MajorAxisLength
#ShapeFactor1(SF1) = MajorAxisLength/Area
samp.B$SF1 <- samp.B$MajorAxisLength / samp.B$Area
#ShapeFactor2(SF2) = MinorAxisLength/Area
samp.B$SF2 <- samp.B$MinorAxisLength / samp.B$Area
#ShapeFactor3(SF3) = Area / (MajorAxisLength/2)^2 * pi
samp.B$SF3 <- samp.B$Area / ((samp.B$MajorAxisLength/2)^2 * pi)
#ShapeFactor4(SF4) = Area / (MajorAxisLength/2)* (MinorAxisLength/2) * pi
samp.B$SF4 <- samp.B$Area / (samp.B$MajorAxisLength/2 * 
                                   samp.B$MinorAxisLength/2 * pi)


col_order<- c("Area", "Perimeter", "MajorAxisLength", "MinorAxisLength", 
              "AspectRatio", "Eccentricity", "ConvexArea", "EquivalentDiameter", 
              "Extent", "Solidity", "Roundness", "Compactness", "SF1", "SF2", 
              "SF3", "SF4")
sampB.dat <- samp.B[,col_order]
```
```{r}

#Aspect Ratio (K)= MajorAxisLength/MinorAxisLength

samp.C$AspectRatio <- samp.C$MajorAxisLength/samp.C$MinorAxisLength
#Solidity = Area/ConvexArea
samp.C$Solidity <- samp.C$Area/samp.C$ConvexArea
#Roundness = 4piArea / Perimeter^2
samp.C$Roundness <- (4 * pi * samp.C$Area) / samp.C$Perimeter^2
#EquivalentDiameter = sqrt(4Area/pit)
samp.C$EquivalentDiameter <- sqrt((4*samp.C$Area) / pi)
#Compactness = Ed(EquivalentDiameter)/L(MajorAxisLength)
samp.C$Compactness <- samp.C$EquivalentDiameter/samp.C$MajorAxisLength
#ShapeFactor1(SF1) = MajorAxisLength/Area
samp.C$SF1 <- samp.C$MajorAxisLength / samp.C$Area
#ShapeFactor2(SF2) = MinorAxisLength/Area
samp.C$SF2 <- samp.C$MinorAxisLength / samp.C$Area
#ShapeFactor3(SF3) = Area / (MajorAxisLength/2)^2 * pi
samp.C$SF3 <- samp.C$Area / ((samp.C$MajorAxisLength/2)^2 * pi)
#ShapeFactor4(SF4) = Area / (MajorAxisLength/2)* (MinorAxisLength/2) * pi
samp.C$SF4 <- samp.C$Area / (samp.C$MajorAxisLength/2 * 
                                   samp.C$MinorAxisLength/2 * pi)


col_order<- c("Area", "Perimeter", "MajorAxisLength", "MinorAxisLength", 
              "AspectRatio", "Eccentricity", "ConvexArea", "EquivalentDiameter", 
              "Extent", "Solidity", "Roundness", "Compactness", "SF1", "SF2", 
              "SF3", "SF4")
sampC.dat <- samp.C[,col_order]
```

## Variable Descriptions

* **Area(A)**: The area of a bean zone and of pixels within its boundaries and is calculated by:

$$A=sum_{r,c\epsilonR} 1$$ 

where,r,c is size of bean region.

* **Perimeter(P)**: Bean circumference is defined as the length of its border.

* **MajorAxisLength(L)**: The distance between the ends of the longest line that can be drawn from a bean.

* **MinorAxisLength(l)**: The longest line that can be drawn from the bean while standing perpendicular to the main axis.

* **Eccentricity (Ec)**:Eccentricity of the ellipse having the same moments as the region.

* **ConvexArea (C)**: Number of pixels in the smallest convex polygon that can contain the area of a bean seed.

* **Extent (Ex)**: The ratio of the pixels in the bounding box to the bean area.

$$Ex=\frac{A}{A_b)$$ 

where,$A_b$ is the Area ofa  bounding rectangle

* **Class**: One of the six bean types (BOMBAY, CALI, DERMASON, HOROZ, SEKER, SIRA)

* **Aspect Ratio(K)**: The relationship between the major axis length (L) and mine axis length (l) and is calculated by:

$$K=\frac{L}{l} $$
* **Solidity(S)**: The ratio of the area of a bean to the area of the smallest convex polygon that fit tightest around the bean. Where Area = $A$ and area of convex polygon = $C$. 

$$S = \frac{A}{C} $$

* **Roundness(R)**: The measure of the sharpness of corners and edges for a given bean. It is calculated as follows where Area = $A$ and Perimeter = $P$.

$$R = \frac{4\pi{A}}{P^2}$$

* **Equivalent Diameter(Ed)**: Calculating an adjusted diameter in relation to the area of a bean. It is calculated as follows where Area = $A$.

$$Ed = \sqrt{\frac{4A}{\pi}}$$

* **Compactness(CO)**: Measure the roundness of the bean using the equivalent diameter in relation to the major axis length. It is calculated as follows were equivalent diameter = $Ed$ and major axis length = $L$.

$$CO = \frac{Ed}{L} $$

* **Shape Factor Features**: Taking the relation of multiple measures to help further clasify each bean:
  
$$ShapeFactor1(SF1) = \frac{L}{A}$$

$$ShapeFactor2(SF2) = \frac{l}{A}$$

$$ShapeFactor3(SF3) = \frac{A}{\frac{L}{2}\frac{L}{2}\pi} $$

$$ShapeFactor3(SF4) = \frac{A}{\frac{L}{2}\frac{l}{2}\pi} $$

## Random Forest Accuracy-Based Importance Variable Reduction

To determine if the neural network model will produce a higher accuracy score by removing unnecessary features from the model, a random forest model was ran to determine which variables are most important in determining a high accuracy score. The importance measure of the random forest will examin each feature in the model and tests the accuracy with the feature in the model and out of the model. The variable with the largest difference gets a higher score and is determine important to predicting the classification.

According to the below table, *Solidity*, *Roundness*, and *SF4* have the least importance in the model with a large jump in scores from the 4th lowest features, *Perimeter*. Therefore, we will test a neural network with all the features and these 3 features removed.

```{r}
rf.mod <- randomForest(as.factor(Class)~.,data=seed.dat)

rf.imp <- data.frame(Variable = row.names(importance(rf.mod)), Importance = importance(rf.mod)[,1])
row.names(rf.imp) <- c()
kable(rf.imp[order(rf.imp$Importance,decreasing=T),],digits=4)
```

## Model 1: Neural Network

### Neural Network - All Variables

Using the *nnet* argument in the *train* function from the **caret** package, we ran a grid of different decay rates, maximum iterations, and number of nodes within the hidden layer. The *nnet* model will only allow for one hidden layer, therefore we were unable to test the model with multiple hidden layers. In addition, this model is unable to select different activation functions and uses a sigmoid linear function.

Tuning the hyper parameters, it was determined that a decay rate of 0.001 and 2 nodes within the hidden layer. In addition, the data was scaled by subtracting the mean from each value in the training data set and dividing it by the standard deviation since each variable is on a different scale.

The metrics that will be used to compare whether feature reduction is necessary are Sensitivity or True Positive Rate which is the True Positives divided by the sum of the True Positives and False Negatives. This shows how accurate the model is at determining the bean clasifications. Specificity or True Negative Rate which is the True Negatives divided by the sum of the True Negatives and False Positives. Finally, Accuracy was used to determine how accurate the model is at predicting both the true positives and true negatives. This is calculated by summing the True Positives and True Negatives divided by all the observations in the training data set.

For the validation method, we used 10 fold cross validation.

According to the table below, the Accuracy was 90.5%.

```{r}
# modelLookup('nnet')
set.seed(42)
ctrl.nn <- trainControl(method='cv', number = 10, summaryFunction = multiClassSummary,classProbs = T, savePredictions = T)
seed.dat$Class <- as.factor(seed.dat.cor$Class)
nn.grid <- expand.grid(size = 2, decay = 0.001)
nnet.fit <- train(Class~., data=seed.dat, trControl=ctrl.nn, method = 'nnet', maxit = 500,
                  tuneGrid = nn.grid, metrix = 'Accuracy', preProcess=c('center','scale'))

# tuneGrid=data.frame(size=c(1:10), decay = c(0.01,0.001,0.0001))
# nn.grid <- expand.grid(size = seq(1,20,by=1), decay = c(0.1,0.01,0.001,0.0001))

nn.results <- nnet.fit$results[,c(1,2,3,6,8,9,10)]
nn.pred <- nnet.fit$pred[,c(1:8)]
```
```{r}
nn.results <- t(nn.results)
colnames(nn.results) <- 'Metrics'
kable(nn.results,digits=4,caption='Result Metrics of NNet - All Variables')
plotnet(nnet.fit,pad_x=0.64,
        y_names = c('BOMBAY','CALI','DERMASON','HOROZ','SEKER','SIRA'),circle_cex=3)
# as.data.frame(coef(nnet.fit$finalModel))
```

```{r, results='hide'}
# modelLookup('nnet')
set.seed(42)
ctrl.nn <- trainControl(method='cv', number = 10, summaryFunction = multiClassSummary,classProbs = T, savePredictions = T)
seed.dat$Class <- as.factor(seed.dat$Class)
nn.grid <- expand.grid(size = 2, decay = 0.001)
nnet.fit <- train(Class~., data=seed.dat, trControl=ctrl.nn, method = 'nnet', maxit = 500,
                  tuneGrid = nn.grid, metrix = 'Accuracy', preProcess=c('center','scale'))

# tuneGrid=data.frame(size=c(1:10), decay = c(0.01,0.001,0.0001))
# nn.grid <- expand.grid(size = seq(1,20,by=1), decay = c(0.1,0.01,0.001,0.0001))

nn.results <- nnet.fit$results[,c(1,2,6,8,9,10)]
nn.pred <- nnet.fit$pred[,c(1:8)]
```

In addition, we determined the accuracy of each of the classes of beans listed below. According to the results, it predicts bombay beans 100% of the time, however it has more trouble predicting the sira bean than any other bean. Observing the photographs of the beans, the sira bean is very similar in shape to the dermason bean, which is has the next lowest accuracy.  

```{r}
nn.acc.dat <- data.frame(Class = c('BOMBAY','CALI','DERMASON','HOROZ','SEKER','SIRA'),Accuracy=NA)
for(i in 1:6){
  temp.dat <- nn.pred[nn.pred[,1]==nn.acc.dat[i,1],]
  nn.acc.dat[i,2] <- sum(ifelse(temp.dat[,1] == temp.dat[,2],1,0))/nrow(temp.dat)
}

kable(nn.acc.dat,digits=4,caption='Accuracy by Class - All Variables')
```

### Neural Network - Random Forest Variable Importance Features

  To determine if the neural network model will produce a higher accuracy score by removing unnecessary features from the model, a random forest model was ran to determine which variables are most important in determining a high accuracy score. The importance measure of the random forest will examin each feature in the model and tests the accuracy with the feature in the model and out of the model. The variable with the largest difference gets a higher score and is determine important to predicting the classification.

  According to the below table, *Solidity*, *Roundness*, and *SF4* have the least importance in the model with a large jump in scores from the 4th lowest features, *Perimeter*. Therefore, we will test a neural network with all the features and these 3 features removed.

```{r}
rf.mod <- randomForest(as.factor(Class)~.,data=seed.dat)

rf.imp <- data.frame(Variable = row.names(importance(rf.mod)), Importance = importance(rf.mod)[,1])
row.names(rf.imp) <- c()
kable(rf.imp[order(rf.imp$Importance,decreasing=T),],digits=4)
```

We ran the neural network model will the 3 least importance variables accoring to the random forest model and followed the same steps as the model above.

According to the results, the random forest variables preform slightly better on the accuracy for the model overall.

```{r, results = 'hide'}
set.seed(42)
seed.dat.rf <- subset(seed.dat,select = -c(Solidity,Roundness,SF4))
seed.dat$Class <- as.factor(seed.dat.rf$Class)

nnet.fit.rf <- train(Class~., data=seed.dat.rf, trControl=ctrl.nn, method = 'nnet', maxit = 500,
                  tuneGrid = nn.grid, metrix = 'Accuracy', preProcess=c('center','scale'))

# tuneGrid=data.frame(size=c(1:10), decay = c(0.01,0.001,0.0001))
# nn.grid <- expand.grid(size = seq(1,20,by=1), decay = c(0.1,0.01,0.001,0.0001))

nn.results.rf <- nnet.fit.rf$results[,c(1,2,6,8,9,10)]
nn.pred.rf <- nnet.fit.rf$pred[,c(1:8)]
```
```{r}
nn.results.rf <- t(nn.results.rf)
colnames(nn.results.rf) <- 'Metrics'
kable(nn.results.rf,digits=3,caption='Result Metrics of NNet - Random Forest Variables')
plotnet(nnet.fit.rf,pad_x=0.64,
        y_names = c('BOMBAY','CALI','DERMASON','HOROZ','SEKER','SIRA'),circle_cex=3)
```

When examining the accuracy of each of the bean classes, we see a slight improvement in the accuracy of the Sira and a slight decline in the Dermason, while the other beans remained fairly constant. Based on the slight improvement of the model accuracy and the reduction in complexity by less features, we used the feature reduction model moving forward to predict the price of the 1 lb. samples of beans.

```{r}
nn.acc.dat.rf <- data.frame(Class = c('BOMBAY','CALI','DERMASON','HOROZ','SEKER','SIRA'),Accuracy=NA)
for(i in 1:6){
  temp.dat <- nn.pred.rf[nn.pred[,1]==nn.acc.dat.rf[i,1],]
  nn.acc.dat.rf[i,2] <- sum(ifelse(temp.dat[,1] == temp.dat[,2],1,0))/nrow(temp.dat)
}

kable(nn.acc.dat.rf,digits=4,caption='Accuracy by Class - Random Forest Variables')
```

### Converting Actuals & Predictions into Price

Using the probablities from our neural net model, and the price per pound and gram per bean given in the assignment documentation, we calculated a cross validated predicted price per bean for our training examples. In addition, we calculated the actual price per pound for each bean using the given class.  

```{r}
bean.tbl <- data.frame(Bean=c('BOMBAY','CALI','DERMASON','HOROZ','SEKER','SIRA'),
                       Gram=c(1.92,0.61,0.28,0.52,0.49,0.38),
                       Price_lb=c(5.56,6.02,1.98,2.43,2.72,5.40))
g_in_lb <- 453.592
bean.tbl$price_bn <- bean.tbl$Gram*bean.tbl$Price/g_in_lb
bean.tbl$lb_bn <- bean.tbl$Gram/g_in_lb

nn.pred.rf <- left_join(nn.pred.rf,bean.tbl, by = c('obs' = 'Bean'))

# Weighted price per bean for predictions
nn.pred.rf$pred_p_bean <- ((nn.pred.rf$BOMBAY*bean.tbl$Price_lb[1]*bean.tbl$Gram[1]/g_in_lb)+
                      (nn.pred.rf$CALI*bean.tbl$Price_lb[2]*bean.tbl$Gram[2]/g_in_lb)+
                      (nn.pred.rf$DERMASON*bean.tbl$Price_lb[3]*bean.tbl$Gram[3]/g_in_lb)+
                      (nn.pred.rf$HOROZ*bean.tbl$Price_lb[4]*bean.tbl$Gram[4]/g_in_lb)+
                      (nn.pred.rf$SEKER*bean.tbl$Price_lb[5]*bean.tbl$Gram[5]/g_in_lb)+
                      (nn.pred.rf$SIRA*bean.tbl$Price_lb[6]*bean.tbl$Gram[6]/g_in_lb))

# Weighted weight in lbs. for predictions
nn.pred.rf$pred_lb     <- ((nn.pred.rf$BOMBAY*bean.tbl$Gram[1]/g_in_lb)+
                           (nn.pred.rf$CALI*bean.tbl$Gram[2]/g_in_lb)+
                           (nn.pred.rf$DERMASON*bean.tbl$Gram[3]/g_in_lb)+
                           (nn.pred.rf$HOROZ*bean.tbl$Gram[4]/g_in_lb)+
                           (nn.pred.rf$SEKER*bean.tbl$Gram[5]/g_in_lb)+
                           (nn.pred.rf$SIRA*bean.tbl$Gram[6]/g_in_lb))

price.dat <- nn.pred.rf[,c(2,11:14)]
```

### Bean Class Predicted Sample Distribution

In order to determine a mean squared error (MSE) for model comparison, we first imported each sample data set and used our model to predict probabilities for each bean classification. We then took these predictions and determine the distribution of each bean class in our sample data.

```{r}
sampA.dat <- read.csv('sampA.newvars.csv')
sampB.dat <- read.csv('sampB.newvars.csv')
sampC.dat <- read.csv('sampC.newvars.csv')

sampA.prob <- predict(nnet.fit.rf,newdata = sampA.dat, type = 'prob')
sampB.prob <- predict(nnet.fit.rf,newdata = sampB.dat, type = 'prob')
sampC.prob <- predict(nnet.fit.rf,newdata = sampC.dat, type = 'prob')
all.prob   <- predict(nnet.fit.rf,newdata = rbind(sampA.dat, sampB.dat, sampC.dat), type = 'prob')
sampA.class <- predict(nnet.fit.rf,newdata = sampA.dat, type = 'raw')
sampB.class <- predict(nnet.fit.rf,newdata = sampB.dat, type = 'raw')
sampC.class <- predict(nnet.fit.rf,newdata = sampC.dat, type = 'raw')
all.class <- predict(nnet.fit.rf,newdata = rbind(sampA.dat,sampB.dat,sampC.dat), type = 'raw')
```

### Distribution of Predicted Observations

According to the below distribution of bean class in each of the samples, Sample A is fairly evenly split between Cali and Seker. Sample B is primarily has a majority of Deramason beans at a proportion of 58% with the next biggest proportion being Sira. Samples C has a majority of Horoz bean with a roughly even split between Deramason and Sira at roughly 17%.

The largest concern in the neural network's ability to predict bean types was between a Dermason and Sira, which is the what Sample B primarily consists of. Therefore, we expect Sample B having the largest margin of error between all the samples.

```{r}
sampA.dist <- as.data.frame(prop.table(table(sampA.class)/length(sampA.class)))
colnames(sampA.dist) <- c('Class','Proportion')
sampB.dist <- as.data.frame(prop.table(table(sampB.class)/length(sampB.class)))
colnames(sampB.dist) <- c('Class','Proportion')
sampC.dist <- as.data.frame(prop.table(table(sampC.class)/length(sampC.class)))
colnames(sampC.dist) <- c('Class','Proportion')
all.dist <- as.data.frame(prop.table(table(all.class)/length(all.class)))
colnames(all.dist) <- c('Class','Proportion')

kable(data.frame(Class=bean.tbl$Bean,A=sampA.dist[,2],B=sampB.dist[,2],
                 C=sampC.dist[,2],All=all.dist[,2]),digits=4,
      caption='Sample Prediction Bean Distribution')
```

### Predicting 1 lb. Samples on Training Data Using Bootstrap w/ Replacement

To determine a comparable MSE and confidence interval in our price per pound we used bootstrpping to resample our training data to create 1 lb samples that had the same distribution of beans as each of our test samples. We sampled a lb of beans 100 times to calculate a MSE and 95% confidence intervals for model comparison.

1,000 samples were drawn to make sure there was enough samples to get a normally distributed set of samples. This is confirmed with the histrograms listed in the appendix as well as a very close mean of median of all the samples.

```{r}
# Boot Function - Samples A-C
boot.fun.t <- function(proportion,x){
    w = 0
    temp.act <- numeric()
    temp.pred <- numeric()
    temp.bean <- character()
    for(row in 1:nrow(x)){
      if(w < 1){
        c <- rmultinom(1,size=1,prob=proportion)
        b <- bean.tbl$Bean[c==T]
        s <- dplyr::sample_n(x[x$obs==b,],1,replace = T)
        temp.act[row] <- as.numeric(s[2])
        temp.pred[row] <- as.numeric(s[4])
        w = w + as.numeric(s[3])
      }
      else break
    }
    temp.dat <- cbind(temp.act,temp.pred)
  return(temp.dat)
}

# Boot Function - All Samples
boot.fun.3 <- function(proportion,x){
    w = 0
    temp.act <- numeric()
    temp.pred <- numeric()
    temp.bean <- character()
    for(row in 1:nrow(x)){
      if(w < 3){
        c <- rmultinom(1,size=1,prob=proportion)
        b <- bean.tbl$Bean[c==T]
        s <- dplyr::sample_n(x[x$obs==b,],1,replace = T)
        temp.act[row] <- as.numeric(s[2])
        temp.pred[row] <- as.numeric(s[4])
        w = w + as.numeric(s[3])
      }
      else break
    }
    temp.dat <- cbind(temp.act,temp.pred)
  return(temp.dat)
}
```
```{r}
set.seed(42)
hist.list <- list()
trials = 1000

sampA.boot <- foreach(1:trials, .combine=rbind) %dopar%{
                result <- boot.fun.t(sampA.dist$Proportion,price.dat)
                c(sum(result[,1]),sum(result[,2]))
                }

sampB.boot <- foreach(1:trials, .combine=rbind) %dopar%{
                result <- boot.fun.t(sampB.dist$Proportion,price.dat)
                c(sum(result[,1]),sum(result[,2]))
                }

sampC.boot <- foreach(1:trials, .combine=rbind) %dopar%{
                result <- boot.fun.t(sampC.dist$Proportion,price.dat)
                c(sum(result[,1]),sum(result[,2]))
                }

all.boot <- foreach(1:trials, .combine=rbind) %dopar%{
                result <- boot.fun.3(all.dist$Proportion,price.dat)
                c(sum(result[,1]),sum(result[,2]))
                }

sampA.df <- data.frame(Metric=c('MSE','Actual Mean Price','Predicted Mean Price','Lower Conf','Median','Upper Conf'),
                 Values=c(mean((sampA.boot[,1]-sampA.boot[,2])^2),mean(sampA.boot[,1]),mean(sampA.boot[,2]),
                          as.numeric(quantile(sampA.boot[,2],c(0.0275,0.5,0.975)))))

sampB.df <- data.frame(Metric=c('MSE','Actual Mean Price','Predicted Mean Price','Lower Conf','Median','Upper Conf'),
                 Values=c(mean((sampB.boot[,1]-sampB.boot[,2])^2),mean(sampB.boot[,1]),mean(sampB.boot[,2]),
                          as.numeric(quantile(sampB.boot[,2],c(0.0275,0.5,0.975)))))

sampC.df <- data.frame(Metric=c('MSE','Actual Mean Price','Predicted Mean Price','Lower Conf','Median','Upper Conf'),
                 Values=c(mean((sampC.boot[,1]-sampC.boot[,2])^2),mean(sampC.boot[,1]),mean(sampC.boot[,2]),
                          as.numeric(quantile(sampC.boot[,2],c(0.0275,0.5,0.975)))))

all.df <- data.frame(Metric=c('MSE','Actual Mean Price','Predicted Mean Price','Lower Conf','Median','Upper Conf'),
                 Values=c(mean((all.boot[,1]-all.boot[,2])^2),mean(all.boot[,1]),mean(all.boot[,2]),
                          as.numeric(quantile(all.boot[,2],c(0.0275,0.5,0.975)))))

hist.list[[1]] <- (qplot(sampA.boot[,2],geom='histogram', binwidth = 0.03,ylab='Count', 
                        xlab = 'Predicted Price', main = 'Price per lb Distribution - Sample A', 
                        fill=I('darkcyan'), col = I('black'))) 

hist.list[[2]] <- (qplot(sampB.boot[,2],geom='histogram', binwidth = 0.02,ylab='Count', 
                        xlab = 'Predicted Price', main = 'Price per lb Distribution - Sample B', 
                        fill=I('lightblue'), col = I('black')))

hist.list[[3]] <- (qplot(sampC.boot[,2],geom='histogram', binwidth = 0.025,ylab='Count', 
                         xlab = 'Predicted Price', main = 'Price per lb Distribution - Sample C', 
                         fill=I('darkgreen'), col = I('black')))

hist.list[[4]] <- (qplot(all.boot[,2],geom='histogram', binwidth = 0.08,ylab='Count', 
                         xlab = 'Predicted Price', main = 'Price per lb Distribution - All Samples',
                         fill=I('deepskyblue2'), col = I('black')))

kable(data.frame(Metric=sampA.df[,1],A=sampA.df[,2],B=sampB.df[,2],C=sampC.df[,2],All=all.df[,2]),
      digits=4,caption='Bootstrap Samples Results - Neural Network')

grid.arrange(hist.list[[1]],hist.list[[2]],hist.list[[3]],hist.list[[4]],nrow=2)
```

## Model 2: Multinomial Logistic Regression

  A Multinomial logistic regression is used to build the model as the target variable has 6 categories. It uses maximum likelihood estimation to evaluate the probability of which category an observation belongs to. The **multinom* method of *nnet* package is used in the *train()* fucntion of *caret* package to build this model. The model is fit with a 10 fold cross validation on scaled data.
  
  We compared metrics like Accuracy, F1 ratio, Sensitivity and Specificity from three logistic regression models built on three different datasets. First dataset(training) has 17 variables including interaction terms and the second dataset(train.vs) has variables selcted from the ggpairs() plot where the correlation between variables is low.
  
  All the metrics from the two models are very similar and we picked the data with variables: Aspectratio, Eccentricity, Extent, Solidity, Roundness, Compactness, SF3,SF4 for further analysis. This model has an accuracy of 90.3%, while the model with all variables has an accuracy of 90.0%.

### Multinomial - All Variables Results

```{r, results='hide'}
set.seed(42)
ctrl.mnom <- trainControl(method='cv', number = 10, summaryFunction = multiClassSummary,classProbs = T, savePredictions = T)
#	parameter for weight decay. Default 0.
mnom.grid <- expand.grid(decay = 0.0001)
mnom.fit <-train(Class~.,data=seed.dat,method = 'multinom', trControl=ctrl.mnom, tuneGrid = mnom.grid,
                  metric='Accuracy',preProcess=c('center','scale'))

mnom.results <- mnom.fit$results[,c(1,2,5,7,8,9)]
mnom.pred <- mnom.fit$pred[,c(1:8)]
mnom.fm<-mnom.fit$finalModel
metrics<-data.frame(Accuracy=round(mnom.results[,c(3)],5),
                    F1=round(mnom.results[,c(4)],5),
                    Sensitivity=round(mnom.results[,c(5)],5),
      Specificity=round(mnom.results[,c(6)],5))
#kable(metrics, caption="10 fold CV Metrics of Training data ")
```

```{r}
mnom.results <- t(mnom.results)
colnames(mnom.results) <- 'Metrics'
kable(mnom.results,digits=4,caption='Result Metrics of Multinomial - All Variables')
```

In addition, we determined the accuracy of each of the classes of beans listed below. According to the results, it predicts bombay beans nearly 100% of the time, however it has more trouble predicting the sira bean than any other bean just like the neural network model.

```{r}
mnom.acc.dat <- data.frame(Class = c('BOMBAY','CALI','DERMASON','HOROZ','SEKER','SIRA'),Accuracy=NA)
for(i in 1:6){
  temp.dat <- mnom.pred[mnom.pred[,1]==mnom.acc.dat[i,1],]
  mnom.acc.dat[i,2] <- sum(ifelse(temp.dat[,1] == temp.dat[,2],1,0))/nrow(temp.dat)
}

kable(mnom.acc.dat,digits=4,caption='Accuracy by Class - All Variables')
```

### Multinomial Logistic Regression - Variable Correlation and Selection

  ggpairs() function is used to make a matrix of scatter plots to visualize and compare the correlation between the variables. From the plot it is clear that area variable is correlated with most of the variables except Aspectratio, Eccentricity, Extent, Solidity, Roundness, Compactness, SF3, SF4.
  
```{r fig.height=12,fig.width=12}
ggpairs(seed.dat)
```
  
### Multinomial - Correlated Feature Reduction

As stated above, using only the features *Area*, *AspectRatio*, *Eccentricity*, *Extent*, *Solidity*, *Roundness*, *Compactness*, *SF3*, and *SF4* improves the accuracy score of the model from 90.0% to 90.3%. 

```{r}
set.seed(42)
cormatrix <- cor(subset(seed.dat,select = -Class))
highcor <- findCorrelation(cormatrix,cutoff = 0.75,verbose=T)
highcor

seed.dat.cor2 <- subset(seed.dat,select = -highcor)
```

```{r, results = 'hide'}
# seed.dat.cor <- subset(seed.dat,select = -highcor)
seed.dat.cor <- subset(seed.dat,select = c(Area, MinorAxisLength, Eccentricity, Compactness, Class))
seed.dat.cor$Class <- as.factor(seed.dat.cor$Class)
set.seed(42)
ctrl.mnom.cor <- trainControl(method='cv', number = 10, summaryFunction = multiClassSummary,classProbs = T, savePredictions = T)
#	parameter for weight decay. Default 0.
mnom.grid.cor <- expand.grid(decay = 0.0001)
mnom.fit.cor <-train(Class~.,data=seed.dat.cor,method = 'multinom', trControl=ctrl.mnom, tuneGrid = mnom.grid,
                  metric='Accuracy',preProcess=c('center','scale'))

mnom.results.cor <- mnom.fit.cor$results[,c(1,2,5,7,8,9)]
mnom.pred.cor <- mnom.fit.cor$pred[,c(1:8)]
mnom.fm.cor <-mnom.fit.cor$finalModel
metrics.cor<-data.frame(Accuracy=round(mnom.results.cor[,c(3)],5),
                    F1=round(mnom.results.cor[,c(4)],5),
                    Sensitivity=round(mnom.results.cor[,c(5)],5),
      Specificity=round(mnom.results.cor[,c(6)],5))
```

```{r}
mnom.results.cor <- t(mnom.results.cor)
colnames(mnom.results.cor) <- 'Metrics'
kable(mnom.results.cor,digits=4,caption='Result Metrics of Multinomial - Noncorrelated Features')
```

When examining the bean calss accuracy, Bombay is now predicted correctly 100% of the time. We also see slight improvements in all other bean types except for Horoz, which decreases from 90.4% accuracy to 89.9% and Seker, which decreases from 91.6% to 91.3%.Due to the improvement in accuracy for the feature reduction model, we used this model moving forward to predict the price of the 1 lb. sample of beans.

```{r}
mnom.acc.dat.cor <- data.frame(Class = c('BOMBAY','CALI','DERMASON','HOROZ','SEKER','SIRA'),Accuracy=NA)
for(i in 1:6){
  temp.dat <- mnom.pred.cor[mnom.pred[,1]==mnom.acc.dat.cor[i,1],]
  mnom.acc.dat.cor[i,2] <- sum(ifelse(temp.dat[,1] == temp.dat[,2],1,0))/nrow(temp.dat)
}

kable(mnom.acc.dat.cor,digits=4,caption='Accuracy by Class - Non-Correlated Features')
```

### Converting Actuals & Predictions into Price

Using the probablities from our multinomial model, and the price per pound and gram per bean given in the assignment documentation, we calculated a cross validated predicted price per bean for our training examples. In addition, we calculated the actual price per pound for each bean using the given class. 

```{r}
mnom.pred.cor <- left_join(mnom.pred.cor,bean.tbl, by = c('obs' = 'Bean'))

# Weighted price per bean for predictions
mnom.pred.cor$pred_p_bean <- ((mnom.pred.cor$BOMBAY*bean.tbl$Price_lb[1]*bean.tbl$Gram[1]/g_in_lb)+
                      (mnom.pred.cor$CALI*bean.tbl$Price_lb[2]*bean.tbl$Gram[2]/g_in_lb)+
                      (mnom.pred.cor$DERMASON*bean.tbl$Price_lb[3]*bean.tbl$Gram[3]/g_in_lb)+
                      (mnom.pred.cor$HOROZ*bean.tbl$Price_lb[4]*bean.tbl$Gram[4]/g_in_lb)+
                      (mnom.pred.cor$SEKER*bean.tbl$Price_lb[5]*bean.tbl$Gram[5]/g_in_lb)+
                      (mnom.pred.cor$SIRA*bean.tbl$Price_lb[6]*bean.tbl$Gram[6]/g_in_lb))

# Weighted weight in lbs. for predictions
mnom.pred.cor$pred_lb     <- ((mnom.pred.cor$BOMBAY*bean.tbl$Gram[1]/g_in_lb)+
                           (mnom.pred.cor$CALI*bean.tbl$Gram[2]/g_in_lb)+
                           (mnom.pred.cor$DERMASON*bean.tbl$Gram[3]/g_in_lb)+
                           (mnom.pred.cor$HOROZ*bean.tbl$Gram[4]/g_in_lb)+
                           (mnom.pred.cor$SEKER*bean.tbl$Gram[5]/g_in_lb)+
                           (mnom.pred.cor$SIRA*bean.tbl$Gram[6]/g_in_lb))

price.dat.mnom <- mnom.pred.cor[,c(2,11:14)]
```

### Bean Class Predicted Sample Distribution

In order to determine a mean squared error (MSE) for model comparison, we first imported each sample data set and used our model to predict probabilities for each bean classification. We then took these predictions and determine the distribution of each bean class in our sample data.

```{r}
sampA.prob.mnom <- predict(mnom.fit.cor,newdata = sampA.dat, type = 'prob')
sampB.prob.mnom <- predict(mnom.fit.cor,newdata = sampB.dat, type = 'prob')
sampC.prob.mnom <- predict(mnom.fit.cor,newdata = sampC.dat, type = 'prob')
all.prob.mnom   <- predict(mnom.fit.cor,newdata = rbind(sampA.dat, sampB.dat, sampC.dat), type = 'prob')
sampA.class.mnom <- predict(mnom.fit.cor,newdata = sampA.dat, type = 'raw')
sampB.class.mnom <- predict(mnom.fit.cor,newdata = sampB.dat, type = 'raw')
sampC.class.mnom <- predict(mnom.fit.cor,newdata = sampC.dat, type = 'raw')
all.class.mnom <- predict(mnom.fit.cor,newdata = rbind(sampA.dat,sampB.dat,sampC.dat), type = 'raw')
```

### Distribution of Predicted Observations

In comparison to the distribution of the neural network predictions, Bombay is predicted the same for all model between the neural network and multinomial models, however there are slight variations in the distribution between the classes of beans. Examining the difference between the 2 models for all samples, the distribution of Cali, Dermason, and Sira increased, while Horoz and Seeker decreased.

```{r}
sampA.dist.mnom <- as.data.frame(prop.table(table(sampA.class.mnom)/length(sampA.class.mnom)))
colnames(sampA.dist.mnom) <- c('Class','Proportion')
sampB.dist.mnom <- as.data.frame(prop.table(table(sampB.class.mnom)/length(sampB.class.mnom)))
colnames(sampB.dist.mnom) <- c('Class','Proportion')
sampC.dist.mnom <- as.data.frame(prop.table(table(sampC.class.mnom)/length(sampC.class.mnom)))
colnames(sampC.dist.mnom) <- c('Class','Proportion')
all.dist.mnom <- as.data.frame(prop.table(table(all.class.mnom)/length(all.class.mnom)))
colnames(all.dist.mnom) <- c('Class','Proportion')

kable(data.frame(Class=bean.tbl$Bean,A=sampA.dist.mnom[,2],B=sampB.dist.mnom[,2],
                 C=sampC.dist.mnom[,2],All=all.dist.mnom[,2]),digits=4,
      caption='Sample Prediction Bean Distribution')
```

### Predicting 1 lb. Samples on Training Data Using Bootstrap w/ Replacement

To determine a comparable MSE and confidence interval in our price per pound we used bootstrpping to resample our training data to create 1 lb samples with the same methodology as the neural network model.

```{r}
set.seed(42)
hist.list.mnom <- list()
trials = 1000

sampA.boot.mnom <- foreach(1:trials, .combine=rbind) %dopar%{
                result <- boot.fun.t(sampA.dist.mnom$Proportion,price.dat.mnom)
                c(sum(result[,1]),sum(result[,2]))
                }

sampB.boot.mnom <- foreach(1:trials, .combine=rbind) %dopar%{
                result <- boot.fun.t(sampB.dist.mnom$Proportion,price.dat.mnom)
                c(sum(result[,1]),sum(result[,2]))
                }

sampC.boot.mnom <- foreach(1:trials, .combine=rbind) %dopar%{
                result <- boot.fun.t(sampC.dist.mnom$Proportion,price.dat.mnom)
                c(sum(result[,1]),sum(result[,2]))
                }

all.boot.mnom <- foreach(1:trials, .combine=rbind) %dopar%{
                result <- boot.fun.3(all.dist.mnom$Proportion,price.dat.mnom)
                c(sum(result[,1]),sum(result[,2]))
                }

sampA.df.mnom <- data.frame(Metric=c('MSE','Actual Mean Price','Predicted Mean Price','Lower Conf','Median','Upper Conf'),
                 Values=c(mean((sampA.boot.mnom[,1]-sampA.boot.mnom[,2])^2),mean(sampA.boot.mnom[,1]),
                          mean(sampA.boot.mnom[,2]),as.numeric(quantile(sampA.boot.mnom[,2],c(0.0275,0.5,0.975)))))

sampB.df.mnom <- data.frame(Metric=c('MSE','Actual Mean Price','Predicted Mean Price','Lower Conf','Median','Upper Conf'),
                 Values=c(mean((sampB.boot.mnom[,1]-sampB.boot.mnom[,2])^2),mean(sampB.boot.mnom[,1]),
                          mean(sampB.boot.mnom[,2]), as.numeric(quantile(sampB.boot.mnom[,2],c(0.0275,0.5,0.975)))))

sampC.df.mnom <- data.frame(Metric=c('MSE','Actual Mean Price','Predicted Mean Price','Lower Conf','Median','Upper Conf'),
                 Values=c(mean((sampC.boot.mnom[,1]-sampC.boot.mnom[,2])^2),mean(sampC.boot.mnom[,1]),
                          mean(sampC.boot.mnom[,2]), as.numeric(quantile(sampC.boot.mnom[,2],c(0.0275,0.5,0.975)))))

all.df.mnom <- data.frame(Metric=c('MSE','Actual Mean Price','Predicted Mean Price','Lower Conf','Median','Upper Conf'),
                 Values=c(mean((all.boot.mnom[,1]-all.boot.mnom[,2])^2),mean(all.boot.mnom[,1]),
                          mean(all.boot.mnom[,2]), as.numeric(quantile(all.boot.mnom[,2],c(0.0275,0.5,0.975)))))

hist.list.mnom[[1]] <- (qplot(sampA.boot.mnom[,2],geom='histogram', binwidth = 0.03,ylab='Count', 
                        xlab = 'Predicted Price', main = 'Price per lb Distribution - Sample A', 
                        fill=I('darkcyan'), col = I('black'))) 

hist.list.mnom[[2]] <- (qplot(sampB.boot.mnom[,2],geom='histogram', binwidth = 0.02,ylab='Count', 
                        xlab = 'Predicted Price', main = 'Price per lb Distribution - Sample B', 
                        fill=I('lightblue'), col = I('black')))

hist.list.mnom[[3]] <- (qplot(sampC.boot.mnom[,2],geom='histogram', binwidth = 0.025,ylab='Count', 
                         xlab = 'Predicted Price', main = 'Price per lb Distribution - Sample C', 
                         fill=I('darkgreen'), col = I('black')))

hist.list.mnom[[4]] <- (qplot(all.boot.mnom[,2],geom='histogram', binwidth = 0.08,ylab='Count', 
                         xlab = 'Predicted Price', main = 'Price per lb Distribution - All Samples',
                         fill=I('deepskyblue2'), col = I('black')))

kable(data.frame(Metric=sampA.df.mnom[,1],A=sampA.df.mnom[,2],B=sampB.df.mnom[,2],C=sampC.df.mnom[,2],All=all.df.mnom[,2]),
      digits=5,caption='Bootstrap Samples Results - Multinomial Logistic Regression')

grid.arrange(hist.list.mnom[[1]],hist.list.mnom[[2]],hist.list.mnom[[3]],hist.list.mnom[[4]],nrow=2)
```

## Model 3: Support Vector Machine (SVM)

USing the same methodology as the other 2 models, a SVM model was run in Caret to determine a comparable cross validated accuracy rate for all the training dataset as well as each bean classification.

### SVM - All Variables

For the caret *train* model using SVM, the cost is the only hyper parameter that can be adjusted. We tried it at multiple cost values and the best fit result was 0.5. The method argument used was the *svmLinear2* method. According to the results of our cross validation, a accuracy score of 90.5% was acquired. 

```{r, results = 'hide'}
#ref: https://topepo.github.io/caret/train-models-by-tag.html#support-vector-machines
set.seed(42)
ctrl.svm <- trainControl(method='cv', number = 10, summaryFunction = multiClassSummary,classProbs = T, savePredictions = T)
seed.dat$Class <- as.factor(seed.dat$Class)
svm.grid <- expand.grid(cost=c(0.5))
svm.fit <- train(Class~., data=seed.dat, trControl=ctrl.svm, method = 'svmLinear2', 
                tuneGrid = svm.grid,
                 metrix = 'Accuracy', preProcess=c('center','scale'))

#best fit results are 0.5, tried 0.001, 0.01, 0.25.,0.5,1
svm.results <- svm.fit$results[,c(1,5,7,8,9)]
svm.pred <- svm.fit$pred[,c(1:8)]
```
```{r}
svm.results <- t(svm.results)
colnames(svm.results) <- 'Metrics'
kable(svm.results,digits=3,caption='Result Metrics of SVM - All Variables')
```

Similar to the other two models, the SVM was able to predict the bombay bean correctly 100% of the time, while it had the hardest time predicting the Sira and Dermason beans with 77.8% and 87.7% accuracy, respectfully.

```{r}
svm.acc.dat <- data.frame(Class = c('BOMBAY','CALI','DERMASON','HOROZ','SEKER','SIRA'),Accuracy=NA)
for(i in 1:6){
  svm.temp.dat <- svm.pred[svm.pred[,1]==svm.acc.dat[i,1],]
  svm.acc.dat[i,2] <- sum(ifelse(svm.temp.dat[,1] == svm.temp.dat[,2],1,0))/nrow(svm.temp.dat)
}

kable(svm.acc.dat,digits=4,caption='Accuracy by Class - All Variables')
```

## SVM - Random Forest Variable Importance Features

As with the neural network model, out of the variable selection/reduction options, the Random forest importance test preformed the best.

As with the all feature test, it had the same overall accuracy at 90.5%

```{r}
set.seed(42)
seed.dat.rf <- subset(seed.dat,select = -c(Solidity,Roundness,SF4))
seed.dat$Class <- as.factor(seed.dat.rf$Class)
 #svm.grid <- expand.grid(cost=c(0.001,0.01,0.1,0.25,0.5,1))
svm.fit.rf <- train(Class~., data=seed.dat.rf, trControl=ctrl.svm, method = 'svmLinear2', 
                  tuneGrid = svm.grid, metrix = 'Accuracy', preProcess=c('center','scale'))

svm.results.rf <- svm.fit.rf$results[,c(1,5,7,8,9)]
svm.pred.rf <- svm.fit.rf$pred[,c(1:8)]
```
```{r}
svm.results.rf <- t(svm.results.rf)
colnames(svm.results.rf) <- 'Metrics'
kable(svm.results.rf,digits=3,caption='Result Metrics of SVM - Random Forest Variables')
```

Examining the accuracy of the bean classification and comparing it to the all feature model, it also clasified all bombay observations correctly, while the Sira bean improved fomr 77.8% to 78.7%. The other variables reamined fairly stable with small variation between the classes.

Since there was a small improvement to the model and also a reduction of complexity and potential noise, we chose the random forest importance test features to move forward in the analysis of SVM.

```{r}
svm.acc.dat.rf <- data.frame(Class = c('BOMBAY','CALI','DERMASON','HOROZ','SEKER','SIRA'),Accuracy=NA)
for(i in 1:6){
  svm.temp.dat <- svm.pred.rf[svm.pred[,1]==svm.acc.dat.rf[i,1],]
  svm.acc.dat.rf[i,2] <- sum(ifelse(svm.temp.dat[,1] == svm.temp.dat[,2],1,0))/nrow(svm.temp.dat)
}

kable(svm.acc.dat.rf,digits=4,caption='Accuracy by Class - Random Forest Variables')
```

## Converting Actuals & Predictions into Price

Using the probablities from our neural net model, and the price per pound and gram per bean given in the assignment documentation, we calculated a cross validated predicted price per bean for our training examples. In addition, we calculated the actual price per pound for each bean using the given class.  

```{r}
bean.tbl <- data.frame(Bean=c('BOMBAY','CALI','DERMASON','HOROZ','SEKER','SIRA'),
                       Gram=c(1.92,0.61,0.28,0.52,0.49,0.38),
                       Price_lb=c(5.56,6.02,1.98,2.43,2.72,5.40))
g_in_lb <- 453.592
bean.tbl$price_bn <- bean.tbl$Gram*bean.tbl$Price/g_in_lb
bean.tbl$lb_bn <- bean.tbl$Gram/g_in_lb

svm.pred.rf <- left_join(svm.pred,bean.tbl, by = c('obs' = 'Bean'))

# Weighted price per bean for predictions
svm.pred.rf$pred_p_bean <- ((svm.pred.rf$BOMBAY*bean.tbl$Price_lb[1]*bean.tbl$Gram[1]/g_in_lb)+
                      (svm.pred.rf$CALI*bean.tbl$Price_lb[2]*bean.tbl$Gram[2]/g_in_lb)+
                      (svm.pred.rf$DERMASON*bean.tbl$Price_lb[3]*bean.tbl$Gram[3]/g_in_lb)+
                      (svm.pred.rf$HOROZ*bean.tbl$Price_lb[4]*bean.tbl$Gram[4]/g_in_lb)+
                      (svm.pred.rf$SEKER*bean.tbl$Price_lb[5]*bean.tbl$Gram[5]/g_in_lb)+
                      (svm.pred.rf$SIRA*bean.tbl$Price_lb[6]*bean.tbl$Gram[6]/g_in_lb))

# Weighted weight in lbs. for predictions
svm.pred.rf$pred_lb     <- ((svm.pred.rf$BOMBAY*bean.tbl$Gram[1]/g_in_lb)+
                           (svm.pred.rf$CALI*bean.tbl$Gram[2]/g_in_lb)+
                           (svm.pred.rf$DERMASON*bean.tbl$Gram[3]/g_in_lb)+
                           (svm.pred.rf$HOROZ*bean.tbl$Gram[4]/g_in_lb)+
                           (svm.pred.rf$SEKER*bean.tbl$Gram[5]/g_in_lb)+
                           (svm.pred.rf$SIRA*bean.tbl$Gram[6]/g_in_lb))

svm.price.dat <- svm.pred.rf[,c(2,11:14)]
```

### Bean Class Predicted Sample Distribution

In order to determine a mean squared error (MSE) for model comparison, we first imported each sample data set and used our model to predict probabilities for each bean classification. We then took these predictions and determine the distribution of each bean class in our sample data.

```{r}
svm.sampA.prob <- predict(svm.fit.rf,newdata = sampA.dat, type = 'prob')
svm.sampB.prob <- predict(svm.fit.rf,newdata = sampB.dat, type = 'prob')
svm.sampC.prob <- predict(svm.fit.rf,newdata = sampC.dat, type = 'prob')
svm.all.class <- predict(svm.fit.rf,newdata = rbind(sampA.dat,sampB.dat,sampC.dat), type = 'raw')

svm.sampA.class <- predict(svm.fit.rf,newdata = sampA.dat, type = 'raw')
svm.sampB.class <- predict(svm.fit.rf,newdata = sampB.dat, type = 'raw')
svm.sampC.class <- predict(svm.fit.rf,newdata = sampC.dat, type = 'raw')
svm.all.prob   <- predict(svm.fit.rf,newdata = rbind(sampA.dat, sampB.dat, sampC.dat), type = 'prob')
```

### Distribution of Predicted Observations

In comparison to the distribution of the neural network and multinomial predictions, Bombay is predicted the same for all models, however there are slight variations in the distribution between the other classes of beans. For most of the bean classification, the SVM model falls in the middel between the neural network and multinomial.

```{r}
svm.sampA.dist <- as.data.frame(prop.table(table(svm.sampA.class)/length(svm.sampA.class)))
colnames(svm.sampA.dist) <- c('Class','Proportion')
svm.sampB.dist <- as.data.frame(prop.table(table(svm.sampB.class)/length(svm.sampB.class)))
colnames(svm.sampB.dist) <- c('Class','Proportion')
svm.sampC.dist <- as.data.frame(prop.table(table(svm.sampC.class)/length(svm.sampC.class)))
colnames(svm.sampC.dist) <- c('Class','Proportion')
svm.all.dist <- as.data.frame(prop.table(table(svm.all.class)/length(svm.all.class)))
colnames(svm.all.dist) <- c('Class','Proportion')


kable(data.frame(Class=bean.tbl$Bean,A=svm.sampA.dist[,2],B=svm.sampB.dist[,2],
                 C=svm.sampC.dist[,2],All=svm.all.dist[,2]),digits=4,
      caption='Sample Prediction Bean Distribution')
```

### Predicting 1 lb. Samples on Training Data Using Bootstrap w/ Replacement

To determine a comparable MSE and confidence interval in our price per pound we used bootstrpping to resample our training data to create 1 lb samples with the same methodology as the neural network model.

```{r}
set.seed(42)
hist.list.svm <- list()
trials = 1000

sampA.boot.svm <- foreach(1:trials, .combine=rbind) %dopar%{
                result <- boot.fun.t(svm.sampA.dist$Proportion,svm.price.dat)
                c(sum(result[,1]),sum(result[,2]))
                }

sampB.boot.svm <- foreach(1:trials, .combine=rbind) %dopar%{
                result <- boot.fun.t(svm.sampB.dist$Proportion,svm.price.dat)
                c(sum(result[,1]),sum(result[,2]))
                }

sampC.boot.svm <- foreach(1:trials, .combine=rbind) %dopar%{
                result <- boot.fun.t(svm.sampC.dist$Proportion,svm.price.dat)
                c(sum(result[,1]),sum(result[,2]))
                }

all.boot.svm <- foreach(1:trials, .combine=rbind) %dopar%{
                result <- boot.fun.3(svm.all.dist$Proportion,svm.price.dat)
                c(sum(result[,1]),sum(result[,2]))
                }

sampA.df.svm <- data.frame(Metric=c('MSE','Actual Mean Price','Predicted Mean Price','Lower Conf','Median','Upper Conf'),
                 Values=c(mean((sampA.boot.svm[,1]-sampA.boot.svm[,2])^2),mean(sampA.boot.svm[,1]),
                          mean(sampA.boot.svm[,2]),as.numeric(quantile(sampA.boot.svm[,2],c(0.0275,0.5,0.975)))))

sampB.df.svm <- data.frame(Metric=c('MSE','Actual Mean Price','Predicted Mean Price','Lower Conf','Median','Upper Conf'),
                 Values=c(mean((sampB.boot.svm[,1]-sampB.boot.svm[,2])^2),mean(sampB.boot.svm[,1]),
                          mean(sampB.boot.svm[,2]), as.numeric(quantile(sampB.boot.svm[,2],c(0.0275,0.5,0.975)))))

sampC.df.svm <- data.frame(Metric=c('MSE','Actual Mean Price','Predicted Mean Price','Lower Conf','Median','Upper Conf'),
                 Values=c(mean((sampC.boot.svm[,1]-sampC.boot.svm[,2])^2),mean(sampC.boot.svm[,1]),
                          mean(sampC.boot.svm[,2]), as.numeric(quantile(sampC.boot.svm[,2],c(0.0275,0.5,0.975)))))

all.df.svm <- data.frame(Metric=c('MSE','Actual Mean Price','Predicted Mean Price','Lower Conf','Median','Upper Conf'),
                 Values=c(mean((all.boot.svm[,1]-all.boot.svm[,2])^2),mean(all.boot.svm[,1]),
                          mean(all.boot.svm[,2]), as.numeric(quantile(all.boot.svm[,2],c(0.0275,0.5,0.975)))))

hist.list.svm[[1]] <- (qplot(sampA.boot.svm[,2],geom='histogram', binwidth = 0.03,ylab='Count', 
                        xlab = 'Predicted Price', main = 'Price per lb Distribution - Sample A', 
                        fill=I('darkcyan'), col = I('black'))) 

hist.list.svm[[2]] <- (qplot(sampB.boot.svm[,2],geom='histogram', binwidth = 0.02,ylab='Count', 
                        xlab = 'Predicted Price', main = 'Price per lb Distribution - Sample B', 
                        fill=I('lightblue'), col = I('black')))

hist.list.svm[[3]] <- (qplot(sampC.boot.svm[,2],geom='histogram', binwidth = 0.025,ylab='Count', 
                         xlab = 'Predicted Price', main = 'Price per lb Distribution - Sample C', 
                         fill=I('darkgreen'), col = I('black')))

hist.list.svm[[4]] <- (qplot(all.boot.svm[,2],geom='histogram', binwidth = 0.08,ylab='Count', 
                         xlab = 'Predicted Price', main = 'Price per lb Distribution - All Samples',
                         fill=I('deepskyblue2'), col = I('black')))

kable(data.frame(Metric=sampA.df.svm[,1],A=sampA.df.svm[,2],B=sampB.df.svm[,2],C=sampC.df.svm[,2],All=all.df.svm[,2]),
      digits=5,caption='Bootstrap Samples Results - SVM')

grid.arrange(hist.list.svm[[1]],hist.list.svm[[2]],hist.list.svm[[3]],hist.list.svm[[4]],nrow=2)
```

## Model Selection for Test Predictions

Comparing the MSE for all 3 samples and all the samples together between all 3 of the models, neural network has the lowest error for samples A and B, however the multinomial model performs better for sample C and all the model together. Therefore, we made our predictions on on the multinomial logistic regression. Even though the neural network has a lower error rate for samples B and C, it is by a very small margin. In addition, the multinomial model is less complex.

```{r}
kable(data.frame(Model=c('Neural Network','Multinomial','SVM'),A=c(as.numeric(sampA.df[1,2]),
                                                                   as.numeric(sampA.df.mnom[1,2]),
                                                                   as.numeric(sampA.df.svm[1,2])),
                                                               B=c(as.numeric(sampB.df[1,2]),
                                                                   as.numeric(sampB.df.mnom[1,2]),
                                                                   as.numeric(sampB.df.svm[1,2])),
                                                               C=c(as.numeric(sampC.df[1,2]),
                                                                   as.numeric(sampC.df.mnom[1,2]),
                                                                   as.numeric(sampC.df.svm[1,2])),
                                                             All=c(as.numeric(all.df[1,2]),
                                                                   as.numeric(all.df.mnom[1,2]),
                                                                   as.numeric(all.df.svm[1,2]))),
      digits = 4, caption='MSE Comparison Between Samples')

```

## Final Predictions on the 1lb Samples and All Samples

The final predictions for using a multinomial logistic regression is \$4.57 for Sample A, \$3.25 for Sample B, \$3.36 for Samplle C, and $11.18 for all the bags combined.

```{r}
sampA.pred <- numeric()
for(i in 1:nrow(sampA.prob.mnom))
sampA.pred[i] <- as.numeric(sampA.prob.mnom[i,])%*%bean.tbl$price_bn
sampA.final.price <- sum(sampA.pred)

sampB.pred <- numeric()
for(i in 1:nrow(sampB.prob.mnom))
sampB.pred[i] <- as.numeric(sampB.prob.mnom[i,])%*%bean.tbl$price_bn
sampB.final.price <- sum(sampB.pred)

sampC.pred <- numeric()
for(i in 1:nrow(sampC.prob.mnom))
sampC.pred[i] <- as.numeric(sampC.prob.mnom[i,])%*%bean.tbl$price_bn
sampC.final.price <- sum(sampC.pred)

all.prob <- rbind(sampA.prob.mnom,sampB.prob.mnom,sampC.prob.mnom)
all.pred <- numeric()
for(i in 1:nrow(all.prob))
all.pred[i] <- as.numeric(all.prob[i,])%*%bean.tbl$price_bn
all.final.price <- sum(all.pred)

kable(data.frame(Sample=c('Sample A','Sample B','Sample C','All Samples'),
                 Price=round(c(sampA.final.price,sampB.final.price,sampC.final.price,
                               all.final.price),2)), caption='Predicted Prices for 1lb Bean Samples')
```
